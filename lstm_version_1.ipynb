{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d4061e",
      "metadata": {
        "id": "85d4061e"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import regularizers\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "import plotly.express as px\n",
        "import scipy.io.wavfile\n",
        "import sys\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41912f39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41912f39",
        "outputId": "738073d4-44cc-4286-dd47-2bce1f5c2062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#这个data只能在这个google folder里用\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "RAV = '/content/drive/MyDrive/Vision_Project/audio_speech_actors_01-24/'\n",
        "dir_list = os.listdir(RAV)\n",
        "\n",
        "emotion = []\n",
        "gender = []\n",
        "path = []\n",
        "feature = []\n",
        "for i in dir_list:\n",
        "    fname = os.listdir(RAV + i)\n",
        "    for f in fname:\n",
        "        part = f.split('.')[0].split('-')\n",
        "        emotion.append(int(part[2]))\n",
        "        temp = int(part[6])\n",
        "        if temp%2 == 0:\n",
        "            temp = \"female\"\n",
        "        else:\n",
        "            temp = \"male\"\n",
        "        gender.append(temp)\n",
        "        path.append(RAV + i + '/' + f)\n",
        "\n",
        "RAV_df = pd.DataFrame(emotion)\n",
        "RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
        "RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n",
        "RAV_df.columns = ['gender','emotion']\n",
        "RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n",
        "RAV_df['source'] = 'RAVDESS'\n",
        "RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
        "RAV_df = RAV_df.drop(['gender'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf1b69b",
      "metadata": {
        "id": "cbf1b69b"
      },
      "outputs": [],
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate = rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
        "\n",
        "# taking any example and checking for techniques.\n",
        "path = np.array(RAV_df.path)[1]\n",
        "data, sample_rate = librosa.load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8336977d",
      "metadata": {
        "id": "8336977d"
      },
      "outputs": [],
      "source": [
        "def extract_features(data):\n",
        "    # ZCR\n",
        "    result = np.array([])\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "    result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "    # Chroma_stft\n",
        "    stft = np.abs(librosa.stft(data))\n",
        "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "    # MFCC\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "    # Root Mean Square Value\n",
        "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "    result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "    # MelSpectogram\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mel)) # stacking horizontally\n",
        "\n",
        "    return result\n",
        "\n",
        "def get_features(path):\n",
        "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "    # without augmentation\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "\n",
        "    # data with noise\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2)) # stacking vertically\n",
        "\n",
        "    # data with stretching and pitching\n",
        "    new_data = stretch(data)\n",
        "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "    res3 = extract_features(data_stretch_pitch)\n",
        "    result = np.vstack((result, res3)) # stacking vertically\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0092e172",
      "metadata": {
        "id": "0092e172"
      },
      "source": [
        "### 6. Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3625636",
      "metadata": {
        "id": "f3625636"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "X, Y = [], []\n",
        "for path, emotion in zip(RAV_df.path, RAV_df.emotion):\n",
        "    feature = get_features(path)\n",
        "    for ele in feature:\n",
        "        X.append(ele)\n",
        "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
        "        Y.append(emotion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b90371d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b90371d",
        "outputId": "ba17b951-e6df-4adb-df87-e8139c788ae3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4320, 4320, (1440,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(X), len(Y), RAV_df.path.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "378b54e0",
      "metadata": {
        "id": "378b54e0"
      },
      "outputs": [],
      "source": [
        "Features = pd.DataFrame(X)\n",
        "Features['labels'] = Y\n",
        "Features.to_csv('features.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a6127c",
      "metadata": {
        "id": "d5a6127c"
      },
      "outputs": [],
      "source": [
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b721fc",
      "metadata": {
        "id": "94b721fc"
      },
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
        "# Save the encoder\n",
        "with open('encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(encoder, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196c70e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196c70e5",
        "outputId": "54a0df65-65de-473a-e15d-4fe6e4e3bc85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3240, 162), (3240, 7), (1080, 162), (1080, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa86186f",
      "metadata": {
        "id": "aa86186f"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
        "# Save the scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3042aa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3042aa3",
        "outputId": "8f240e4d-4e13-4602-f4ae-0aec233ca131"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3240, 162, 1), (3240, 7), (1080, 162, 1), (1080, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FSXUvXBjcBkZ",
      "metadata": {
        "id": "FSXUvXBjcBkZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers, optimizers, callbacks\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04beb81a",
      "metadata": {
        "id": "04beb81a"
      },
      "source": [
        "### 7. Modelling :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a8ba7e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a8ba7e2",
        "outputId": "c31b1bbc-2793-45d4-8557-685779c9cb5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 162, 64)           16896     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50375 (196.78 KB)\n",
            "Trainable params: 50375 (196.78 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "MODEL = Sequential()\n",
        "MODEL.add(layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
        "MODEL.add(layers.LSTM(64))\n",
        "MODEL.add(layers.Dense(7, activation='softmax'))\n",
        "\n",
        "MODEL.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(MODEL.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=4, min_lr=0.000001)\n",
        "history=MODEL.fit(x_train, y_train, batch_size=50, epochs=200, validation_data=(x_test, y_test), callbacks=[rlrp])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfmfoowJuDgg",
        "outputId": "8d22aac2-9432-465a-894c-4b8331206cdc"
      },
      "id": "rfmfoowJuDgg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "65/65 [==============================] - 9s 29ms/step - loss: 1.8227 - accuracy: 0.2676 - val_loss: 1.7795 - val_accuracy: 0.3009 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.7598 - accuracy: 0.2985 - val_loss: 1.7611 - val_accuracy: 0.3120 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.7471 - accuracy: 0.3012 - val_loss: 1.7477 - val_accuracy: 0.3083 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.7272 - accuracy: 0.3164 - val_loss: 1.7507 - val_accuracy: 0.3231 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.7272 - accuracy: 0.3130 - val_loss: 1.7050 - val_accuracy: 0.3343 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.7117 - accuracy: 0.3201 - val_loss: 1.7333 - val_accuracy: 0.3370 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.6975 - accuracy: 0.3296 - val_loss: 1.6846 - val_accuracy: 0.3370 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.6852 - accuracy: 0.3361 - val_loss: 1.7178 - val_accuracy: 0.3315 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.6981 - accuracy: 0.3330 - val_loss: 1.6653 - val_accuracy: 0.3472 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "65/65 [==============================] - 2s 30ms/step - loss: 1.6822 - accuracy: 0.3389 - val_loss: 1.6810 - val_accuracy: 0.3463 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "65/65 [==============================] - 2s 35ms/step - loss: 1.6624 - accuracy: 0.3488 - val_loss: 1.6680 - val_accuracy: 0.3519 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "65/65 [==============================] - 2s 25ms/step - loss: 1.6666 - accuracy: 0.3503 - val_loss: 1.6610 - val_accuracy: 0.3500 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "65/65 [==============================] - 2s 24ms/step - loss: 1.6746 - accuracy: 0.3349 - val_loss: 1.6689 - val_accuracy: 0.3463 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "65/65 [==============================] - 2s 28ms/step - loss: 1.6436 - accuracy: 0.3503 - val_loss: 1.6650 - val_accuracy: 0.3556 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "65/65 [==============================] - 1s 23ms/step - loss: 1.6516 - accuracy: 0.3506 - val_loss: 1.6357 - val_accuracy: 0.3620 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "65/65 [==============================] - 1s 23ms/step - loss: 1.6607 - accuracy: 0.3441 - val_loss: 1.6876 - val_accuracy: 0.3407 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.6522 - accuracy: 0.3562 - val_loss: 1.6510 - val_accuracy: 0.3565 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.6457 - accuracy: 0.3549 - val_loss: 1.7374 - val_accuracy: 0.3287 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.6489 - accuracy: 0.3537 - val_loss: 1.6506 - val_accuracy: 0.3546 - lr: 4.0000e-04\n",
            "Epoch 20/200\n",
            "65/65 [==============================] - 1s 23ms/step - loss: 1.6207 - accuracy: 0.3679 - val_loss: 1.6294 - val_accuracy: 0.3630 - lr: 4.0000e-04\n",
            "Epoch 21/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.6113 - accuracy: 0.3716 - val_loss: 1.6698 - val_accuracy: 0.3556 - lr: 4.0000e-04\n",
            "Epoch 22/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.6212 - accuracy: 0.3676 - val_loss: 1.6289 - val_accuracy: 0.3574 - lr: 4.0000e-04\n",
            "Epoch 23/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.6065 - accuracy: 0.3796 - val_loss: 1.6210 - val_accuracy: 0.3593 - lr: 4.0000e-04\n",
            "Epoch 24/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5999 - accuracy: 0.3716 - val_loss: 1.6193 - val_accuracy: 0.3639 - lr: 4.0000e-04\n",
            "Epoch 25/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.6002 - accuracy: 0.3741 - val_loss: 1.6206 - val_accuracy: 0.3602 - lr: 4.0000e-04\n",
            "Epoch 26/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5974 - accuracy: 0.3728 - val_loss: 1.6551 - val_accuracy: 0.3528 - lr: 4.0000e-04\n",
            "Epoch 27/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5973 - accuracy: 0.3762 - val_loss: 1.6154 - val_accuracy: 0.3685 - lr: 4.0000e-04\n",
            "Epoch 28/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5842 - accuracy: 0.3840 - val_loss: 1.6146 - val_accuracy: 0.3657 - lr: 4.0000e-04\n",
            "Epoch 29/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5909 - accuracy: 0.3799 - val_loss: 1.6193 - val_accuracy: 0.3759 - lr: 4.0000e-04\n",
            "Epoch 30/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5776 - accuracy: 0.3892 - val_loss: 1.6226 - val_accuracy: 0.3657 - lr: 4.0000e-04\n",
            "Epoch 31/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5828 - accuracy: 0.3877 - val_loss: 1.6581 - val_accuracy: 0.3565 - lr: 4.0000e-04\n",
            "Epoch 32/200\n",
            "65/65 [==============================] - 1s 20ms/step - loss: 1.5757 - accuracy: 0.3802 - val_loss: 1.6103 - val_accuracy: 0.3741 - lr: 4.0000e-04\n",
            "Epoch 33/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.5701 - accuracy: 0.3870 - val_loss: 1.6000 - val_accuracy: 0.3713 - lr: 4.0000e-04\n",
            "Epoch 34/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.5834 - accuracy: 0.3855 - val_loss: 1.6618 - val_accuracy: 0.3454 - lr: 4.0000e-04\n",
            "Epoch 35/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.5664 - accuracy: 0.3873 - val_loss: 1.5913 - val_accuracy: 0.3731 - lr: 4.0000e-04\n",
            "Epoch 36/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5508 - accuracy: 0.3917 - val_loss: 1.6074 - val_accuracy: 0.3713 - lr: 4.0000e-04\n",
            "Epoch 37/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5538 - accuracy: 0.3920 - val_loss: 1.5913 - val_accuracy: 0.3769 - lr: 4.0000e-04\n",
            "Epoch 38/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5553 - accuracy: 0.3972 - val_loss: 1.6352 - val_accuracy: 0.3593 - lr: 4.0000e-04\n",
            "Epoch 39/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5608 - accuracy: 0.3833 - val_loss: 1.5838 - val_accuracy: 0.3704 - lr: 4.0000e-04\n",
            "Epoch 40/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.5418 - accuracy: 0.3926 - val_loss: 1.5940 - val_accuracy: 0.3787 - lr: 4.0000e-04\n",
            "Epoch 41/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5401 - accuracy: 0.3938 - val_loss: 1.5849 - val_accuracy: 0.3806 - lr: 4.0000e-04\n",
            "Epoch 42/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5294 - accuracy: 0.3969 - val_loss: 1.6474 - val_accuracy: 0.3620 - lr: 4.0000e-04\n",
            "Epoch 43/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5323 - accuracy: 0.4034 - val_loss: 1.5778 - val_accuracy: 0.3852 - lr: 4.0000e-04\n",
            "Epoch 44/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5320 - accuracy: 0.4052 - val_loss: 1.6433 - val_accuracy: 0.3657 - lr: 4.0000e-04\n",
            "Epoch 45/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.5287 - accuracy: 0.4083 - val_loss: 1.5734 - val_accuracy: 0.4102 - lr: 4.0000e-04\n",
            "Epoch 46/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.5254 - accuracy: 0.4068 - val_loss: 1.5873 - val_accuracy: 0.3778 - lr: 4.0000e-04\n",
            "Epoch 47/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.5127 - accuracy: 0.4204 - val_loss: 1.6053 - val_accuracy: 0.3787 - lr: 4.0000e-04\n",
            "Epoch 48/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.5229 - accuracy: 0.4080 - val_loss: 1.5642 - val_accuracy: 0.3935 - lr: 4.0000e-04\n",
            "Epoch 49/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5011 - accuracy: 0.4120 - val_loss: 1.5375 - val_accuracy: 0.4157 - lr: 4.0000e-04\n",
            "Epoch 50/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.5138 - accuracy: 0.4151 - val_loss: 1.5394 - val_accuracy: 0.4139 - lr: 4.0000e-04\n",
            "Epoch 51/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5042 - accuracy: 0.4219 - val_loss: 1.5585 - val_accuracy: 0.4167 - lr: 4.0000e-04\n",
            "Epoch 52/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.4970 - accuracy: 0.4262 - val_loss: 1.5546 - val_accuracy: 0.4028 - lr: 4.0000e-04\n",
            "Epoch 53/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4862 - accuracy: 0.4373 - val_loss: 1.5614 - val_accuracy: 0.3963 - lr: 4.0000e-04\n",
            "Epoch 54/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.5019 - accuracy: 0.4204 - val_loss: 1.5352 - val_accuracy: 0.4120 - lr: 4.0000e-04\n",
            "Epoch 55/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.4868 - accuracy: 0.4309 - val_loss: 1.5396 - val_accuracy: 0.4019 - lr: 4.0000e-04\n",
            "Epoch 56/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4774 - accuracy: 0.4343 - val_loss: 1.5477 - val_accuracy: 0.4102 - lr: 4.0000e-04\n",
            "Epoch 57/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.4698 - accuracy: 0.4343 - val_loss: 1.5658 - val_accuracy: 0.3917 - lr: 4.0000e-04\n",
            "Epoch 58/200\n",
            "65/65 [==============================] - 1s 23ms/step - loss: 1.4649 - accuracy: 0.4410 - val_loss: 1.5538 - val_accuracy: 0.4093 - lr: 4.0000e-04\n",
            "Epoch 59/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.4810 - accuracy: 0.4293 - val_loss: 1.5443 - val_accuracy: 0.4222 - lr: 4.0000e-04\n",
            "Epoch 60/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.4693 - accuracy: 0.4414 - val_loss: 1.5461 - val_accuracy: 0.4204 - lr: 4.0000e-04\n",
            "Epoch 61/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.4689 - accuracy: 0.4346 - val_loss: 1.5355 - val_accuracy: 0.4037 - lr: 4.0000e-04\n",
            "Epoch 62/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4578 - accuracy: 0.4417 - val_loss: 1.5461 - val_accuracy: 0.4120 - lr: 4.0000e-04\n",
            "Epoch 63/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4527 - accuracy: 0.4463 - val_loss: 1.5345 - val_accuracy: 0.4185 - lr: 4.0000e-04\n",
            "Epoch 64/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4481 - accuracy: 0.4515 - val_loss: 1.5792 - val_accuracy: 0.4009 - lr: 4.0000e-04\n",
            "Epoch 65/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4537 - accuracy: 0.4432 - val_loss: 1.5319 - val_accuracy: 0.4176 - lr: 4.0000e-04\n",
            "Epoch 66/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.4443 - accuracy: 0.4534 - val_loss: 1.5315 - val_accuracy: 0.4093 - lr: 4.0000e-04\n",
            "Epoch 67/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.4461 - accuracy: 0.4475 - val_loss: 1.6635 - val_accuracy: 0.3676 - lr: 4.0000e-04\n",
            "Epoch 68/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.4695 - accuracy: 0.4373 - val_loss: 1.5277 - val_accuracy: 0.4222 - lr: 4.0000e-04\n",
            "Epoch 69/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.4543 - accuracy: 0.4509 - val_loss: 1.5589 - val_accuracy: 0.4028 - lr: 4.0000e-04\n",
            "Epoch 70/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.4474 - accuracy: 0.4512 - val_loss: 1.5270 - val_accuracy: 0.4139 - lr: 4.0000e-04\n",
            "Epoch 71/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.4134 - accuracy: 0.4685 - val_loss: 1.5247 - val_accuracy: 0.4241 - lr: 1.6000e-04\n",
            "Epoch 72/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.4041 - accuracy: 0.4719 - val_loss: 1.5250 - val_accuracy: 0.4231 - lr: 1.6000e-04\n",
            "Epoch 73/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.3976 - accuracy: 0.4725 - val_loss: 1.5208 - val_accuracy: 0.4333 - lr: 1.6000e-04\n",
            "Epoch 74/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3946 - accuracy: 0.4741 - val_loss: 1.5254 - val_accuracy: 0.4139 - lr: 1.6000e-04\n",
            "Epoch 75/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3913 - accuracy: 0.4793 - val_loss: 1.5340 - val_accuracy: 0.4120 - lr: 1.6000e-04\n",
            "Epoch 76/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3881 - accuracy: 0.4778 - val_loss: 1.5198 - val_accuracy: 0.4213 - lr: 1.6000e-04\n",
            "Epoch 77/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3829 - accuracy: 0.4775 - val_loss: 1.5170 - val_accuracy: 0.4194 - lr: 1.6000e-04\n",
            "Epoch 78/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3859 - accuracy: 0.4772 - val_loss: 1.5174 - val_accuracy: 0.4250 - lr: 1.6000e-04\n",
            "Epoch 79/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3813 - accuracy: 0.4827 - val_loss: 1.5229 - val_accuracy: 0.4157 - lr: 1.6000e-04\n",
            "Epoch 80/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3834 - accuracy: 0.4843 - val_loss: 1.5225 - val_accuracy: 0.4250 - lr: 1.6000e-04\n",
            "Epoch 81/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3744 - accuracy: 0.4877 - val_loss: 1.5110 - val_accuracy: 0.4194 - lr: 1.6000e-04\n",
            "Epoch 82/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3692 - accuracy: 0.4799 - val_loss: 1.5350 - val_accuracy: 0.4102 - lr: 1.6000e-04\n",
            "Epoch 83/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.3863 - accuracy: 0.4738 - val_loss: 1.5227 - val_accuracy: 0.4204 - lr: 1.6000e-04\n",
            "Epoch 84/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.3731 - accuracy: 0.4873 - val_loss: 1.5221 - val_accuracy: 0.4194 - lr: 1.6000e-04\n",
            "Epoch 85/200\n",
            "65/65 [==============================] - 2s 24ms/step - loss: 1.3690 - accuracy: 0.4932 - val_loss: 1.5298 - val_accuracy: 0.4250 - lr: 1.6000e-04\n",
            "Epoch 86/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.3705 - accuracy: 0.4904 - val_loss: 1.5276 - val_accuracy: 0.4259 - lr: 1.6000e-04\n",
            "Epoch 87/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3582 - accuracy: 0.4901 - val_loss: 1.5307 - val_accuracy: 0.4083 - lr: 1.6000e-04\n",
            "Epoch 88/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3610 - accuracy: 0.4895 - val_loss: 1.5400 - val_accuracy: 0.4065 - lr: 1.6000e-04\n",
            "Epoch 89/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3557 - accuracy: 0.4929 - val_loss: 1.5182 - val_accuracy: 0.4324 - lr: 1.6000e-04\n",
            "Epoch 90/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3692 - accuracy: 0.4824 - val_loss: 1.5228 - val_accuracy: 0.4306 - lr: 1.6000e-04\n",
            "Epoch 91/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3586 - accuracy: 0.4901 - val_loss: 1.5326 - val_accuracy: 0.4213 - lr: 1.6000e-04\n",
            "Epoch 92/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3617 - accuracy: 0.4904 - val_loss: 1.5225 - val_accuracy: 0.4167 - lr: 1.6000e-04\n",
            "Epoch 93/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3557 - accuracy: 0.4954 - val_loss: 1.5594 - val_accuracy: 0.4037 - lr: 1.6000e-04\n",
            "Epoch 94/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3312 - accuracy: 0.5049 - val_loss: 1.5303 - val_accuracy: 0.4157 - lr: 6.4000e-05\n",
            "Epoch 95/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3252 - accuracy: 0.5034 - val_loss: 1.5159 - val_accuracy: 0.4259 - lr: 6.4000e-05\n",
            "Epoch 96/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.3271 - accuracy: 0.5056 - val_loss: 1.5280 - val_accuracy: 0.4194 - lr: 6.4000e-05\n",
            "Epoch 97/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.3269 - accuracy: 0.5022 - val_loss: 1.5203 - val_accuracy: 0.4167 - lr: 6.4000e-05\n",
            "Epoch 98/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.3201 - accuracy: 0.5077 - val_loss: 1.5242 - val_accuracy: 0.4185 - lr: 6.4000e-05\n",
            "Epoch 99/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.3188 - accuracy: 0.5136 - val_loss: 1.5179 - val_accuracy: 0.4287 - lr: 6.4000e-05\n",
            "Epoch 100/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3149 - accuracy: 0.5130 - val_loss: 1.5178 - val_accuracy: 0.4250 - lr: 6.4000e-05\n",
            "Epoch 101/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3137 - accuracy: 0.5080 - val_loss: 1.5234 - val_accuracy: 0.4315 - lr: 6.4000e-05\n",
            "Epoch 102/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3126 - accuracy: 0.5136 - val_loss: 1.5253 - val_accuracy: 0.4231 - lr: 6.4000e-05\n",
            "Epoch 103/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3139 - accuracy: 0.5108 - val_loss: 1.5369 - val_accuracy: 0.4111 - lr: 6.4000e-05\n",
            "Epoch 104/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3110 - accuracy: 0.5111 - val_loss: 1.5150 - val_accuracy: 0.4269 - lr: 6.4000e-05\n",
            "Epoch 105/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3096 - accuracy: 0.5139 - val_loss: 1.5345 - val_accuracy: 0.4111 - lr: 6.4000e-05\n",
            "Epoch 106/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.3121 - accuracy: 0.5093 - val_loss: 1.5177 - val_accuracy: 0.4194 - lr: 6.4000e-05\n",
            "Epoch 107/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3109 - accuracy: 0.5074 - val_loss: 1.5205 - val_accuracy: 0.4176 - lr: 6.4000e-05\n",
            "Epoch 108/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3066 - accuracy: 0.5154 - val_loss: 1.5244 - val_accuracy: 0.4259 - lr: 6.4000e-05\n",
            "Epoch 109/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.3074 - accuracy: 0.5123 - val_loss: 1.5262 - val_accuracy: 0.4241 - lr: 6.4000e-05\n",
            "Epoch 110/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.3043 - accuracy: 0.5133 - val_loss: 1.5253 - val_accuracy: 0.4231 - lr: 6.4000e-05\n",
            "Epoch 111/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.3011 - accuracy: 0.5136 - val_loss: 1.5221 - val_accuracy: 0.4250 - lr: 6.4000e-05\n",
            "Epoch 112/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.2955 - accuracy: 0.5201 - val_loss: 1.5585 - val_accuracy: 0.4120 - lr: 6.4000e-05\n",
            "Epoch 113/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2966 - accuracy: 0.5198 - val_loss: 1.5202 - val_accuracy: 0.4250 - lr: 6.4000e-05\n",
            "Epoch 114/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.3014 - accuracy: 0.5188 - val_loss: 1.5335 - val_accuracy: 0.4231 - lr: 6.4000e-05\n",
            "Epoch 115/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2950 - accuracy: 0.5157 - val_loss: 1.5338 - val_accuracy: 0.4231 - lr: 6.4000e-05\n",
            "Epoch 116/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2944 - accuracy: 0.5210 - val_loss: 1.5274 - val_accuracy: 0.4231 - lr: 6.4000e-05\n",
            "Epoch 117/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2890 - accuracy: 0.5225 - val_loss: 1.5293 - val_accuracy: 0.4324 - lr: 6.4000e-05\n",
            "Epoch 118/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2877 - accuracy: 0.5185 - val_loss: 1.5338 - val_accuracy: 0.4259 - lr: 6.4000e-05\n",
            "Epoch 119/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2897 - accuracy: 0.5238 - val_loss: 1.5376 - val_accuracy: 0.4157 - lr: 6.4000e-05\n",
            "Epoch 120/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2923 - accuracy: 0.5201 - val_loss: 1.5424 - val_accuracy: 0.4130 - lr: 6.4000e-05\n",
            "Epoch 121/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2919 - accuracy: 0.5222 - val_loss: 1.5214 - val_accuracy: 0.4398 - lr: 6.4000e-05\n",
            "Epoch 122/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2861 - accuracy: 0.5225 - val_loss: 1.5343 - val_accuracy: 0.4139 - lr: 6.4000e-05\n",
            "Epoch 123/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2844 - accuracy: 0.5265 - val_loss: 1.5264 - val_accuracy: 0.4213 - lr: 6.4000e-05\n",
            "Epoch 124/200\n",
            "65/65 [==============================] - 2s 24ms/step - loss: 1.2829 - accuracy: 0.5235 - val_loss: 1.5545 - val_accuracy: 0.4111 - lr: 6.4000e-05\n",
            "Epoch 125/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2925 - accuracy: 0.5170 - val_loss: 1.5331 - val_accuracy: 0.4250 - lr: 6.4000e-05\n",
            "Epoch 126/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2878 - accuracy: 0.5247 - val_loss: 1.5303 - val_accuracy: 0.4315 - lr: 6.4000e-05\n",
            "Epoch 127/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2809 - accuracy: 0.5228 - val_loss: 1.5388 - val_accuracy: 0.4269 - lr: 6.4000e-05\n",
            "Epoch 128/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2809 - accuracy: 0.5231 - val_loss: 1.5301 - val_accuracy: 0.4194 - lr: 6.4000e-05\n",
            "Epoch 129/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2763 - accuracy: 0.5269 - val_loss: 1.5457 - val_accuracy: 0.4148 - lr: 6.4000e-05\n",
            "Epoch 130/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2814 - accuracy: 0.5210 - val_loss: 1.5328 - val_accuracy: 0.4213 - lr: 6.4000e-05\n",
            "Epoch 131/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2822 - accuracy: 0.5281 - val_loss: 1.5264 - val_accuracy: 0.4315 - lr: 6.4000e-05\n",
            "Epoch 132/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2748 - accuracy: 0.5302 - val_loss: 1.5370 - val_accuracy: 0.4148 - lr: 6.4000e-05\n",
            "Epoch 133/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2756 - accuracy: 0.5278 - val_loss: 1.5265 - val_accuracy: 0.4278 - lr: 6.4000e-05\n",
            "Epoch 134/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.2788 - accuracy: 0.5235 - val_loss: 1.5330 - val_accuracy: 0.4157 - lr: 6.4000e-05\n",
            "Epoch 135/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.2733 - accuracy: 0.5312 - val_loss: 1.5359 - val_accuracy: 0.4194 - lr: 6.4000e-05\n",
            "Epoch 136/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2677 - accuracy: 0.5290 - val_loss: 1.5413 - val_accuracy: 0.4296 - lr: 6.4000e-05\n",
            "Epoch 137/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2720 - accuracy: 0.5238 - val_loss: 1.5328 - val_accuracy: 0.4324 - lr: 6.4000e-05\n",
            "Epoch 138/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2697 - accuracy: 0.5256 - val_loss: 1.5327 - val_accuracy: 0.4296 - lr: 6.4000e-05\n",
            "Epoch 139/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2661 - accuracy: 0.5321 - val_loss: 1.5450 - val_accuracy: 0.4259 - lr: 6.4000e-05\n",
            "Epoch 140/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2704 - accuracy: 0.5315 - val_loss: 1.5390 - val_accuracy: 0.4315 - lr: 6.4000e-05\n",
            "Epoch 141/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2701 - accuracy: 0.5269 - val_loss: 1.5345 - val_accuracy: 0.4241 - lr: 6.4000e-05\n",
            "Epoch 142/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2718 - accuracy: 0.5272 - val_loss: 1.5321 - val_accuracy: 0.4259 - lr: 6.4000e-05\n",
            "Epoch 143/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2648 - accuracy: 0.5299 - val_loss: 1.5280 - val_accuracy: 0.4250 - lr: 6.4000e-05\n",
            "Epoch 144/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2601 - accuracy: 0.5318 - val_loss: 1.5309 - val_accuracy: 0.4287 - lr: 6.4000e-05\n",
            "Epoch 145/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2623 - accuracy: 0.5343 - val_loss: 1.5402 - val_accuracy: 0.4269 - lr: 6.4000e-05\n",
            "Epoch 146/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2645 - accuracy: 0.5349 - val_loss: 1.5289 - val_accuracy: 0.4389 - lr: 6.4000e-05\n",
            "Epoch 147/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.2649 - accuracy: 0.5306 - val_loss: 1.5326 - val_accuracy: 0.4241 - lr: 6.4000e-05\n",
            "Epoch 148/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2639 - accuracy: 0.5312 - val_loss: 1.5383 - val_accuracy: 0.4176 - lr: 6.4000e-05\n",
            "Epoch 149/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2551 - accuracy: 0.5358 - val_loss: 1.5299 - val_accuracy: 0.4296 - lr: 2.5600e-05\n",
            "Epoch 150/200\n",
            "65/65 [==============================] - 2s 24ms/step - loss: 1.2583 - accuracy: 0.5358 - val_loss: 1.5338 - val_accuracy: 0.4315 - lr: 2.5600e-05\n",
            "Epoch 151/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2492 - accuracy: 0.5389 - val_loss: 1.5342 - val_accuracy: 0.4241 - lr: 2.5600e-05\n",
            "Epoch 152/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2465 - accuracy: 0.5389 - val_loss: 1.5363 - val_accuracy: 0.4269 - lr: 2.5600e-05\n",
            "Epoch 153/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2455 - accuracy: 0.5463 - val_loss: 1.5369 - val_accuracy: 0.4287 - lr: 2.5600e-05\n",
            "Epoch 154/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2466 - accuracy: 0.5380 - val_loss: 1.5314 - val_accuracy: 0.4315 - lr: 2.5600e-05\n",
            "Epoch 155/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2425 - accuracy: 0.5417 - val_loss: 1.5316 - val_accuracy: 0.4296 - lr: 2.5600e-05\n",
            "Epoch 156/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2419 - accuracy: 0.5441 - val_loss: 1.5328 - val_accuracy: 0.4259 - lr: 2.5600e-05\n",
            "Epoch 157/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2417 - accuracy: 0.5454 - val_loss: 1.5399 - val_accuracy: 0.4269 - lr: 2.5600e-05\n",
            "Epoch 158/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2448 - accuracy: 0.5383 - val_loss: 1.5343 - val_accuracy: 0.4204 - lr: 2.5600e-05\n",
            "Epoch 159/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2419 - accuracy: 0.5441 - val_loss: 1.5347 - val_accuracy: 0.4259 - lr: 2.5600e-05\n",
            "Epoch 160/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.2398 - accuracy: 0.5438 - val_loss: 1.5350 - val_accuracy: 0.4269 - lr: 2.5600e-05\n",
            "Epoch 161/200\n",
            "65/65 [==============================] - 1s 20ms/step - loss: 1.2406 - accuracy: 0.5432 - val_loss: 1.5322 - val_accuracy: 0.4361 - lr: 2.5600e-05\n",
            "Epoch 162/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.2422 - accuracy: 0.5435 - val_loss: 1.5328 - val_accuracy: 0.4315 - lr: 2.5600e-05\n",
            "Epoch 163/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.2430 - accuracy: 0.5398 - val_loss: 1.5339 - val_accuracy: 0.4380 - lr: 2.5600e-05\n",
            "Epoch 164/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.2376 - accuracy: 0.5432 - val_loss: 1.5363 - val_accuracy: 0.4250 - lr: 2.5600e-05\n",
            "Epoch 165/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.2375 - accuracy: 0.5435 - val_loss: 1.5295 - val_accuracy: 0.4315 - lr: 2.5600e-05\n",
            "Epoch 166/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.2387 - accuracy: 0.5414 - val_loss: 1.5310 - val_accuracy: 0.4324 - lr: 2.5600e-05\n",
            "Epoch 167/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.2381 - accuracy: 0.5469 - val_loss: 1.5306 - val_accuracy: 0.4269 - lr: 2.5600e-05\n",
            "Epoch 168/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2380 - accuracy: 0.5478 - val_loss: 1.5342 - val_accuracy: 0.4250 - lr: 2.5600e-05\n",
            "Epoch 169/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2320 - accuracy: 0.5451 - val_loss: 1.5293 - val_accuracy: 0.4324 - lr: 1.0240e-05\n",
            "Epoch 170/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2317 - accuracy: 0.5472 - val_loss: 1.5324 - val_accuracy: 0.4324 - lr: 1.0240e-05\n",
            "Epoch 171/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2312 - accuracy: 0.5451 - val_loss: 1.5304 - val_accuracy: 0.4287 - lr: 1.0240e-05\n",
            "Epoch 172/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.2313 - accuracy: 0.5444 - val_loss: 1.5280 - val_accuracy: 0.4287 - lr: 1.0240e-05\n",
            "Epoch 173/200\n",
            "65/65 [==============================] - 1s 20ms/step - loss: 1.2299 - accuracy: 0.5494 - val_loss: 1.5311 - val_accuracy: 0.4296 - lr: 1.0240e-05\n",
            "Epoch 174/200\n",
            "65/65 [==============================] - 2s 24ms/step - loss: 1.2303 - accuracy: 0.5491 - val_loss: 1.5276 - val_accuracy: 0.4269 - lr: 1.0240e-05\n",
            "Epoch 175/200\n",
            "65/65 [==============================] - 1s 19ms/step - loss: 1.2301 - accuracy: 0.5500 - val_loss: 1.5335 - val_accuracy: 0.4315 - lr: 1.0240e-05\n",
            "Epoch 176/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2298 - accuracy: 0.5475 - val_loss: 1.5318 - val_accuracy: 0.4296 - lr: 1.0240e-05\n",
            "Epoch 177/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2306 - accuracy: 0.5485 - val_loss: 1.5294 - val_accuracy: 0.4287 - lr: 1.0240e-05\n",
            "Epoch 178/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2286 - accuracy: 0.5469 - val_loss: 1.5299 - val_accuracy: 0.4287 - lr: 1.0240e-05\n",
            "Epoch 179/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2284 - accuracy: 0.5472 - val_loss: 1.5286 - val_accuracy: 0.4306 - lr: 1.0240e-05\n",
            "Epoch 180/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2301 - accuracy: 0.5466 - val_loss: 1.5292 - val_accuracy: 0.4306 - lr: 1.0240e-05\n",
            "Epoch 181/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2315 - accuracy: 0.5497 - val_loss: 1.5292 - val_accuracy: 0.4278 - lr: 1.0240e-05\n",
            "Epoch 182/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2292 - accuracy: 0.5469 - val_loss: 1.5308 - val_accuracy: 0.4306 - lr: 1.0240e-05\n",
            "Epoch 183/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2290 - accuracy: 0.5494 - val_loss: 1.5294 - val_accuracy: 0.4296 - lr: 1.0240e-05\n",
            "Epoch 184/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2263 - accuracy: 0.5506 - val_loss: 1.5302 - val_accuracy: 0.4315 - lr: 4.0960e-06\n",
            "Epoch 185/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.2264 - accuracy: 0.5497 - val_loss: 1.5301 - val_accuracy: 0.4333 - lr: 4.0960e-06\n",
            "Epoch 186/200\n",
            "65/65 [==============================] - 1s 23ms/step - loss: 1.2258 - accuracy: 0.5485 - val_loss: 1.5297 - val_accuracy: 0.4324 - lr: 4.0960e-06\n",
            "Epoch 187/200\n",
            "65/65 [==============================] - 2s 23ms/step - loss: 1.2255 - accuracy: 0.5481 - val_loss: 1.5303 - val_accuracy: 0.4333 - lr: 4.0960e-06\n",
            "Epoch 188/200\n",
            "65/65 [==============================] - 1s 17ms/step - loss: 1.2251 - accuracy: 0.5485 - val_loss: 1.5300 - val_accuracy: 0.4306 - lr: 4.0960e-06\n",
            "Epoch 189/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2249 - accuracy: 0.5481 - val_loss: 1.5310 - val_accuracy: 0.4333 - lr: 4.0960e-06\n",
            "Epoch 190/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2253 - accuracy: 0.5485 - val_loss: 1.5307 - val_accuracy: 0.4333 - lr: 4.0960e-06\n",
            "Epoch 191/200\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 1.2256 - accuracy: 0.5494 - val_loss: 1.5296 - val_accuracy: 0.4324 - lr: 4.0960e-06\n",
            "Epoch 192/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2248 - accuracy: 0.5478 - val_loss: 1.5304 - val_accuracy: 0.4333 - lr: 4.0960e-06\n",
            "Epoch 193/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2249 - accuracy: 0.5491 - val_loss: 1.5299 - val_accuracy: 0.4315 - lr: 4.0960e-06\n",
            "Epoch 194/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2252 - accuracy: 0.5503 - val_loss: 1.5303 - val_accuracy: 0.4315 - lr: 4.0960e-06\n",
            "Epoch 195/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2246 - accuracy: 0.5478 - val_loss: 1.5302 - val_accuracy: 0.4306 - lr: 4.0960e-06\n",
            "Epoch 196/200\n",
            "65/65 [==============================] - 1s 16ms/step - loss: 1.2245 - accuracy: 0.5522 - val_loss: 1.5310 - val_accuracy: 0.4315 - lr: 4.0960e-06\n",
            "Epoch 197/200\n",
            "65/65 [==============================] - 1s 18ms/step - loss: 1.2248 - accuracy: 0.5485 - val_loss: 1.5308 - val_accuracy: 0.4306 - lr: 4.0960e-06\n",
            "Epoch 198/200\n",
            "65/65 [==============================] - 1s 23ms/step - loss: 1.2242 - accuracy: 0.5491 - val_loss: 1.5306 - val_accuracy: 0.4343 - lr: 4.0960e-06\n",
            "Epoch 199/200\n",
            "65/65 [==============================] - 1s 22ms/step - loss: 1.2241 - accuracy: 0.5512 - val_loss: 1.5309 - val_accuracy: 0.4315 - lr: 4.0960e-06\n",
            "Epoch 200/200\n",
            "65/65 [==============================] - 1s 21ms/step - loss: 1.2245 - accuracy: 0.5491 - val_loss: 1.5309 - val_accuracy: 0.4324 - lr: 4.0960e-06\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 107620,
          "sourceId": 256618,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30674,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1052.963631,
      "end_time": "2024-03-29T03:32:04.047193",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-03-29T03:14:31.083562",
      "version": "2.5.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}